{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "“EC500 Opt4ML PA2 SOLUTIONS.ipynb”的副本",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RockZhang29/EC601_Project/blob/main/%E2%80%9CEC500_Opt4ML_PA2_SOLUTIONS_ipynb%E2%80%9D%E7%9A%84%E5%89%AF%E6%9C%AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-rlonHRgR8e"
      },
      "source": [
        "# Optimization for Machine Learning PA 2\n",
        "\n",
        "This homework assignment investigates implementing some variants of Adam. We will be testing your optimizers on a simplified implementation of [GPT](https://github.com/openai/gpt-3) based on the [minGPT](https://github.com/karpathy/minGPT) repository by Andrej Karpathy. This is a model that takes as input a sequence of characters from a text file and attempts to predict the next character. This can be used to generate novel text by starting with a seed text string, and then repeatedly using the model to generate another character.\n",
        "\n",
        "There is only one place you need to write code in this notebook, for questions **1a**, **1b**.\n",
        "\n",
        "To turn in this homework: download as .ipynb (File -> download as .ipynb). Make the filename YOURNAME_PA2.ipynb and send via email attachment to opt4mlclass+program2@gmail.com with your name and PA2 in the subject line. Your submission should have **all cells run to completion**. The final error of the best optimizer should be **less than 1.7** (and the best optimizer should be the one from question 1b).\n",
        "\n",
        "This homework is **DUE on Monday 3/22 at 11:59 pm**.\n",
        "\n",
        "# Tips\n",
        "* You will need a GPU for this assignment. When using google colab, go to runtime->change runtime type and make sure that the type is set to GPU.\n",
        "\n",
        "* You may decrease the number of training epochs while debugging, but please set it back to 20 and run again before submission.\n",
        "\n",
        "* Study the provided AdaGrad implementation closely, it introduces a few pytorch functions that may be useful. You should check the documentation for these functions to see what they do.\n",
        "\n",
        "* You may occasionally need to restart the runtime (runtime->restart runtime). Sometimes the GPUs don't release memory properly, and sometimes the progress bars get a little messed up."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wzx2UJoruvmS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f67523e-cca9-418b-c97b-26cca85bdd77"
      },
      "source": [
        "# grab the data and auxiliary code. Feel free to checkout the git repo to see\n",
        "# what the model code will do.\n",
        "\n",
        "!git clone https://github.com/acutkosky/opt4mlPA2.git\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'opt4mlPA2' already exists and is not an empty directory.\n",
            "--2021-03-30 18:21:03--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt.1’\n",
            "\n",
            "input.txt.1         100%[===================>]   1.06M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2021-03-30 18:21:03 (16.4 MB/s) - ‘input.txt.1’ saved [1115394/1115394]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8nfb6rQvKsi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "406964a6-8269-40e0-a7c6-8230d461f5b1"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input.txt  input.txt.1\topt4mlPA2  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZNclArxur0O"
      },
      "source": [
        "# set up logging\n",
        "import logging\n",
        "logging.basicConfig(\n",
        "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
        "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "        level=logging.INFO,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPNiLzaiur0P"
      },
      "source": [
        "# make deterministic\n",
        "from opt4mlPA2.mingpt.utils import set_seed\n",
        "set_seed(42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6-pL9SDur0Q"
      },
      "source": [
        "#import all the things\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from torch.optim import Optimizer\n",
        "\n",
        "import math\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "from opt4mlPA2.mingpt.model import GPT, GPTConfig\n",
        "from opt4mlPA2.mingpt.utils import sample\n",
        "from opt4mlPA2.mingpt.trainer import Trainer, TrainerConfig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayO9eAReur0R"
      },
      "source": [
        "# define a dataset class to process the textfile in pytorch.\n",
        "\n",
        "class CharDataset(Dataset):\n",
        "\n",
        "    def __init__(self, data, block_size):\n",
        "        chars = list(set(data))\n",
        "        data_size, vocab_size = len(data), len(chars)\n",
        "        print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
        "        \n",
        "        self.stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "        self.itos = { i:ch for i,ch in enumerate(chars) }\n",
        "        self.block_size = block_size\n",
        "        self.vocab_size = vocab_size\n",
        "        self.data = data\n",
        "    \n",
        "    def __len__(self):\n",
        "        return math.ceil(len(self.data) / (self.block_size + 1))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # we're actually going to \"cheat\" and pick a spot in the dataset at random\n",
        "        i = np.random.randint(0, len(self.data) - (self.block_size + 1))\n",
        "        chunk = self.data[i:i+self.block_size+1]\n",
        "        dix = [self.stoi[s] for s in chunk]\n",
        "        x = torch.tensor(dix[:-1], dtype=torch.long)\n",
        "        y = torch.tensor(dix[1:], dtype=torch.long)\n",
        "        return x, y\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QsEKavRur0S"
      },
      "source": [
        "# the \"block size\" is the number of characters the model takes as input.\n",
        "# in this case, it can look at up to 128 characters when predicting the next\n",
        "# character.\n",
        "block_size = 128 # spatial extent of the model for its context"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kk2o1Loxur0T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f65638e-e4d4-4674-bc50-cd56d67ffb1b"
      },
      "source": [
        "# you can download this file at https://github.com/karpathy/char-rnn/blob/master/data/tinyshakespeare/input.txt\n",
        "text = open('input.txt', 'r').read() # don't worry we won't run out of file handles\n",
        "train_dataset = CharDataset(text, block_size) # one line of poem is roughly 50 characters"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data has 1115394 characters, 65 unique.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_2gClq9lDj_"
      },
      "source": [
        "# AdaGrad implementation\n",
        "This is a simple adagrad implementation. You can also checkout the official pytorch implementation [here](https://github.com/pytorch/pytorch/blob/master/torch/optim/adagrad.py).\n",
        "\n",
        "Take particular note of the functions `addcmul_` and `addcdiv_`. You may want to look up what they do for use in your solution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpqcVSyeIpp4"
      },
      "source": [
        "class AdaGrad(Optimizer):\n",
        "  def __init__(self, params, lr=1.0, betas=(0.9,0.999), decouple=False, debias=True):\n",
        "    # betas are ignored, but we keep them in the function signature so that it is the same as\n",
        "    # the adam variants.\n",
        "    super(AdaGrad, self).__init__(params, {'lr': lr, 'beta1': betas[0], 'beta2': betas[1], 'weight_decay': 0.0})\n",
        "\n",
        "\n",
        "    for group in self.param_groups:\n",
        "      for p in group['params']:\n",
        "        state = self.state[p]\n",
        "        state['step'] = 0\n",
        "        state['v'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
        "\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def step(self, closure=None):\n",
        "    # in this class, and also usually in practice, closure will always be None.\n",
        "    loss = None\n",
        "    epsilon = 1e-8\n",
        "    if closure is not None:\n",
        "      with torch.enable_grad():\n",
        "        loss = closure()\n",
        "\n",
        "    for group in self.param_groups:\n",
        "      lr = group['lr']\n",
        "      beta1 = group['beta1']\n",
        "      beta2 = group['beta2']\n",
        "      weight_decay = group['weight_decay']\n",
        "\n",
        "      # it is common practice to call the model parameters p in code.\n",
        "      # in class we follow more closely analytical conventions, in which the\n",
        "      # parameters are often called w for weights.\n",
        "      for p in group['params']:\n",
        "        if p.grad is None:\n",
        "          continue\n",
        "\n",
        "        if weight_decay != 0.0:\n",
        "          p.grad.add_(p, alpha=weight_decay)\n",
        "        \n",
        "        # Update the iteration counter (again, this is not actually used in this algorithm)\n",
        "        state = self.state[p]\n",
        "        state['step'] += 1\n",
        "\n",
        "\n",
        "        state['v'].addcmul_(p.grad, p.grad, value=1.0)\n",
        "\n",
        "        p.addcdiv_(p.grad, torch.sqrt(state['v']).add_(epsilon), value=-lr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIFg4v93Z_q8"
      },
      "source": [
        "# QUESTION 1a\n",
        "\n",
        "Implement the [AdamW](https://openreview.net/pdf?id=Bkg6RiCqY7) update *without* using the debiasing terms. AdamW performs the following (per-coordinate) update:\n",
        "\n",
        "$$\n",
        "w_{t+1} = w_t - \\eta_t\\left(\\frac{\\hat m_t}{\\sqrt{\\hat v_t} +\\epsilon} + \\lambda w_t\\right)\n",
        "$$\n",
        "where $\\hat m_t$ and $\\hat v_t$ are generated the same way as in the standard [Adam](https://openreview.net/pdf?id=Bkg6RiCqY7) update, and $\\lambda$ is an extra \"weight decay\" parameter provided to the optimizer. \n",
        "\n",
        "Ordinarily, \"weight decay\" is another word for L2 regularization. That is, the loss is modified to:\n",
        "$$\n",
        "\\mathcal{L}(w) + \\frac{\\lambda}{2}\\|w\\|^2\n",
        "$$\n",
        "This means that we could implement weight decay by changing the gradient to $\\nabla \\mathcal{L}(w) + \\lambda w$. The idea behind AdamW is that the weight-decay term is in some sense \"well-understood\" and should not be included in the $v_t$ and $A_t$ statistics that are being used to understand the more mysterious loss surface $\\mathcal{L}(w)$. See the linked paper for more details and full pseudocode.\n",
        "\n",
        "In your implementation, you should use the raw $m_t$ and $v_t$ values without applying the debiasing terms discussed in the papers and class.\n",
        "\n",
        "# QUESTION 1b\n",
        "\n",
        "Upgrade your debias-free AdamW implementation to use the `use_norm_scaling` argument of the `__init__` method. When this argument is `True`, you should scale the learning rate by the norm of the weights *for the given pytorch variable*. That is, for each variable $p$ you will replace the learning rate $\\eta_t$ at time $t$ with $\\|p\\|\\eta_t$ in the update:\n",
        "$$\n",
        "w_{t+1}[i] = w_t[i] - \\|w_t\\|_2\\eta_t\\left(\\frac{m_t[i]}{\\sqrt{v_t[i]} +\\epsilon} + \\lambda w_t[i]\\right)\n",
        "$$\n",
        "When the `use_norm_scaling` argument is false, simply perform the update from question 1a.\n",
        "\n",
        "This learning rate heuristic is inspired by a similar proposal for use with normalized updates in the [LARS](https://arxiv.org/abs/1708.03888) optimizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNST4-QUKyTc"
      },
      "source": [
        "\n",
        "class AdamW_bias(Optimizer):\n",
        "  def __init__(self, params, lr=1.0, betas=(0.9,0.999), use_norm_scaling=False):\n",
        "    super(AdamW_bias, self).__init__(params, {'lr': lr, 'beta1': betas[0], 'beta2': betas[1], 'weight_decay': 0.0})\n",
        "\n",
        "\n",
        "    epsilon = 1e-8\n",
        "\n",
        "    self.use_norm_scaling = use_norm_scaling\n",
        "\n",
        "    for group in self.param_groups:\n",
        "      for p in group['params']:\n",
        "        ## YOUR CODE HERE ##\n",
        "\n",
        "\n",
        "\n",
        "        state = self.state[p]\n",
        "        state['step'] = 0\n",
        "        state['v'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
        "        state['A'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
        "\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def step(self, closure=None):\n",
        "    # in this class, and also usually in practice, closure will always be None.\n",
        "    loss = None\n",
        "    epsilon = 1e-8\n",
        "    if closure is not None:\n",
        "      with torch.enable_grad():\n",
        "        loss = closure()\n",
        "      \n",
        "    ## YOUR CODE HERE ##\n",
        "\n",
        "    for group in self.param_groups:\n",
        "      # this lr value is set up by the base class constructor if the 'params'\n",
        "      # argument to __init__ is a list.\n",
        "      lr = group['lr']\n",
        "      beta1 = group['beta1']\n",
        "      beta2 = group['beta2']\n",
        "      weight_decay = group['weight_decay']\n",
        "\n",
        "      # it is common practice to call the model parameters p in code.\n",
        "      # in class we follow more closely analytical conventions, in which the\n",
        "      # parameters are often called w for weights.\n",
        "      for p in group['params']:\n",
        "        if p.grad is None:\n",
        "          continue\n",
        "        \n",
        "        # Update the iteration counter (again, this is not actually used in this algorithm)\n",
        "        state = self.state[p]\n",
        "        state['step'] += 1\n",
        "\n",
        "        state['v'].mul_(beta1).add_(p.grad, alpha=1.0-beta1)\n",
        "\n",
        "        state['A'].mul_(beta2).addcmul_(p.grad, p.grad, value=1.0-beta2)\n",
        "\n",
        "        p.mul_(1.0- lr * weight_decay)\n",
        "\n",
        "        denom = torch.sqrt(state['A']).add_(epsilon)\n",
        "\n",
        "        if self.use_norm_scaling:\n",
        "          denom.div_(torch.norm(p))\n",
        "\n",
        "        p.addcdiv_(state['v'], denom, value=-lr)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XG99O9rXur0U"
      },
      "source": [
        "# generate the configuration for the model. These parameters specify\n",
        "# the neural network architecture we will be using. It is not necessary\n",
        "# to understand this.\n",
        "mconf = GPTConfig(train_dataset.vocab_size, train_dataset.block_size,\n",
        "                  n_layer=8, n_head=8, n_embd=128)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPP9QE90E1Aa"
      },
      "source": [
        "# generate training configurations for each of the optimizers. We will be testing\n",
        "# adagrad\n",
        "# adam (official pytorch implementation)\n",
        "# adamw (official pytorch implementation)\n",
        "# your optimizer both with and without the norm_scaling flag set.\n",
        "def adamw_bias_factory(params, lr, betas):\n",
        "  return AdamW_bias(params, lr, betas)\n",
        "\n",
        "def adamw_bias_norm_scaling_factory(params, lr, betas):\n",
        "  return AdamW_bias(params, lr, betas, use_norm_scaling=True)\n",
        "\n",
        "optimizers = {\n",
        "    'adagrad': AdaGrad, \n",
        "    'adam': torch.optim.Adam, \n",
        "    'adamw': torch.optim.AdamW, \n",
        "    'adamw_bias': adamw_bias_factory, \n",
        "    'adamw_bias_norm_scaling': adamw_bias_norm_scaling_factory\n",
        "  }\n",
        "\n",
        "training_configs = {}\n",
        "\n",
        "for name, opt in optimizers.items():\n",
        "# construct a training config: this sets the learning rate, batch size, number \n",
        "# of epochs ect for each optimizer. warmup_tokens and final_tokens are parameters\n",
        "# used to setup a warm-up and decay learning rate scheduler.\n",
        "  training_configs[name] = TrainerConfig(max_epochs=20, batch_size=256, learning_rate=6e-4, optimizer=opt,\n",
        "                        lr_decay=True, warmup_tokens=512*20, final_tokens=200*len(train_dataset)*block_size,\n",
        "                        num_workers=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGDVS4_ZQNRV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d730129-30cf-4bfe-b269-9639826ca421"
      },
      "source": [
        "# train a model on each optimizer, keeping track of the best-performing one.\n",
        "losses = {}\n",
        "min_loss = float('inf')\n",
        "best_model = None\n",
        "best_optimizer = None\n",
        "for name, tconf in training_configs.items():\n",
        "  print(\"training new model with optimizer: {}\".format(name))\n",
        "  model = GPT(mconf)\n",
        "  trainer = Trainer(model, train_dataset, None, tconf)\n",
        "  train_loss = trainer.train()\n",
        "  losses[name] = train_loss\n",
        "  train_dataset = CharDataset(text, block_size) # one line of poem is roughly 50 characters\n",
        "  print(\"final epoch train loss: {}\".format(train_loss))\n",
        "  if train_loss < min_loss:\n",
        "    best_model = model\n",
        "    best_optimizer = name\n",
        "    min_loss = train_loss\n",
        "\n",
        "print(\"best optimizer: {} with loss: {}\".format(best_optimizer, min_loss))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "03/30/2021 18:21:04 - INFO - opt4mlPA2.mingpt.model -   number of parameters: 1.619456e+06\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training new model with optimizer: adagrad\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "epoch 1 iter 33: train loss 3.30675. lr 5.999637e-04: 100%|██████████| 34/34 [00:18<00:00,  1.84it/s]\n",
            "epoch 2 iter 33: train loss 3.09272. lr 5.998533e-04: 100%|██████████| 34/34 [00:18<00:00,  1.85it/s]\n",
            "epoch 3 iter 33: train loss 2.98458. lr 5.996690e-04: 100%|██████████| 34/34 [00:18<00:00,  1.85it/s]\n",
            "epoch 4 iter 33: train loss 2.91123. lr 5.994107e-04: 100%|██████████| 34/34 [00:18<00:00,  1.85it/s]\n",
            "epoch 5 iter 33: train loss 2.86231. lr 5.990785e-04: 100%|██████████| 34/34 [00:18<00:00,  1.85it/s]\n",
            "epoch 6 iter 33: train loss 2.82469. lr 5.986726e-04: 100%|██████████| 34/34 [00:18<00:00,  1.85it/s]\n",
            "epoch 7 iter 33: train loss 2.79915. lr 5.981929e-04: 100%|██████████| 34/34 [00:18<00:00,  1.86it/s]\n",
            "epoch 8 iter 33: train loss 2.77555. lr 5.976397e-04: 100%|██████████| 34/34 [00:18<00:00,  1.86it/s]\n",
            "epoch 9 iter 33: train loss 2.75878. lr 5.970130e-04: 100%|██████████| 34/34 [00:18<00:00,  1.86it/s]\n",
            "epoch 10 iter 33: train loss 2.74522. lr 5.963130e-04: 100%|██████████| 34/34 [00:18<00:00,  1.85it/s]\n",
            "epoch 11 iter 33: train loss 2.73397. lr 5.955399e-04: 100%|██████████| 34/34 [00:18<00:00,  1.85it/s]\n",
            "epoch 12 iter 33: train loss 2.72575. lr 5.946939e-04: 100%|██████████| 34/34 [00:18<00:00,  1.85it/s]\n",
            "epoch 13 iter 33: train loss 2.71618. lr 5.937751e-04: 100%|██████████| 34/34 [00:18<00:00,  1.85it/s]\n",
            "epoch 14 iter 33: train loss 2.71067. lr 5.927839e-04: 100%|██████████| 34/34 [00:18<00:00,  1.85it/s]\n",
            "epoch 15 iter 33: train loss 2.70422. lr 5.917204e-04: 100%|██████████| 34/34 [00:18<00:00,  1.85it/s]\n",
            "epoch 16 iter 33: train loss 2.69928. lr 5.905849e-04: 100%|██████████| 34/34 [00:18<00:00,  1.85it/s]\n",
            "epoch 17 iter 33: train loss 2.69526. lr 5.893777e-04: 100%|██████████| 34/34 [00:18<00:00,  1.85it/s]\n",
            "epoch 18 iter 33: train loss 2.69161. lr 5.880992e-04: 100%|██████████| 34/34 [00:18<00:00,  1.85it/s]\n",
            "epoch 19 iter 33: train loss 2.68785. lr 5.867495e-04: 100%|██████████| 34/34 [00:18<00:00,  1.85it/s]\n",
            "epoch 20 iter 33: train loss 2.68486. lr 5.853291e-04: 100%|██████████| 34/34 [00:18<00:00,  1.85it/s]\n",
            "03/30/2021 18:27:16 - INFO - opt4mlPA2.mingpt.model -   number of parameters: 1.619456e+06\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "data has 1115394 characters, 65 unique.\n",
            "final epoch train loss: 2.681358638931722\n",
            "training new model with optimizer: adam\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 1 iter 33: train loss 2.86608. lr 5.999637e-04: 100%|██████████| 34/34 [00:18<00:00,  1.84it/s]\n",
            "epoch 2 iter 33: train loss 2.68452. lr 5.998533e-04: 100%|██████████| 34/34 [00:18<00:00,  1.84it/s]\n",
            "epoch 3 iter 33: train loss 2.66244. lr 5.996690e-04: 100%|██████████| 34/34 [00:18<00:00,  1.84it/s]\n",
            "epoch 4 iter 33: train loss 2.62594. lr 5.994107e-04: 100%|██████████| 34/34 [00:18<00:00,  1.84it/s]\n",
            "epoch 5 iter 33: train loss 2.62216. lr 5.990785e-04: 100%|██████████| 34/34 [00:18<00:00,  1.85it/s]\n",
            "epoch 6 iter 33: train loss 2.61633. lr 5.986726e-04: 100%|██████████| 34/34 [00:18<00:00,  1.84it/s]\n",
            "epoch 7 iter 33: train loss 2.61278. lr 5.981929e-04: 100%|██████████| 34/34 [00:18<00:00,  1.84it/s]\n",
            "epoch 8 iter 33: train loss 2.60831. lr 5.976397e-04: 100%|██████████| 34/34 [00:18<00:00,  1.84it/s]\n",
            "epoch 9 iter 33: train loss 2.60348. lr 5.970130e-04: 100%|██████████| 34/34 [00:18<00:00,  1.84it/s]\n",
            "epoch 10 iter 33: train loss 2.60206. lr 5.963130e-04: 100%|██████████| 34/34 [00:18<00:00,  1.84it/s]\n",
            "epoch 11 iter 33: train loss 2.59599. lr 5.955399e-04: 100%|██████████| 34/34 [00:18<00:00,  1.84it/s]\n",
            "epoch 12 iter 33: train loss 2.59234. lr 5.946939e-04: 100%|██████████| 34/34 [00:18<00:00,  1.85it/s]\n",
            "epoch 13 iter 33: train loss 2.58947. lr 5.937751e-04: 100%|██████████| 34/34 [00:18<00:00,  1.84it/s]\n",
            "epoch 14 iter 33: train loss 2.58594. lr 5.927839e-04: 100%|██████████| 34/34 [00:18<00:00,  1.84it/s]\n",
            "epoch 15 iter 33: train loss 2.58197. lr 5.917204e-04: 100%|██████████| 34/34 [00:18<00:00,  1.84it/s]\n",
            "epoch 16 iter 33: train loss 2.57984. lr 5.905849e-04: 100%|██████████| 34/34 [00:18<00:00,  1.84it/s]\n",
            "epoch 17 iter 33: train loss 2.57747. lr 5.893777e-04: 100%|██████████| 34/34 [00:18<00:00,  1.84it/s]\n",
            "epoch 18 iter 33: train loss 2.57468. lr 5.880992e-04: 100%|██████████| 34/34 [00:18<00:00,  1.85it/s]\n",
            "epoch 19 iter 33: train loss 2.57195. lr 5.867495e-04: 100%|██████████| 34/34 [00:18<00:00,  1.84it/s]\n",
            "epoch 20 iter 33: train loss 2.56986. lr 5.853291e-04: 100%|██████████| 34/34 [00:18<00:00,  1.84it/s]\n",
            "03/30/2021 18:33:28 - INFO - opt4mlPA2.mingpt.model -   number of parameters: 1.619456e+06\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "data has 1115394 characters, 65 unique.\n",
            "final epoch train loss: 2.5682345348245956\n",
            "training new model with optimizer: adamw\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 1 iter 33: train loss 2.75190. lr 5.999637e-04: 100%|██████████| 34/34 [00:18<00:00,  1.84it/s]\n",
            "epoch 2 iter 33: train loss 2.53971. lr 5.998533e-04: 100%|██████████| 34/34 [00:18<00:00,  1.84it/s]\n",
            "epoch 3 iter 33: train loss 2.45697. lr 5.996690e-04: 100%|██████████| 34/34 [00:18<00:00,  1.84it/s]\n",
            "epoch 4 iter 33: train loss 2.40154. lr 5.994107e-04: 100%|██████████| 34/34 [00:18<00:00,  1.84it/s]\n",
            "epoch 5 iter 33: train loss 2.35111. lr 5.990785e-04: 100%|██████████| 34/34 [00:18<00:00,  1.84it/s]\n",
            "epoch 6 iter 33: train loss 2.29229. lr 5.986726e-04: 100%|██████████| 34/34 [00:18<00:00,  1.84it/s]\n",
            "epoch 7 iter 33: train loss 2.23596. lr 5.981929e-04: 100%|██████████| 34/34 [00:18<00:00,  1.84it/s]\n",
            "epoch 8 iter 33: train loss 2.18943. lr 5.976397e-04: 100%|██████████| 34/34 [00:18<00:00,  1.83it/s]\n",
            "epoch 9 iter 33: train loss 2.13250. lr 5.970130e-04: 100%|██████████| 34/34 [00:18<00:00,  1.84it/s]\n",
            "epoch 10 iter 33: train loss 2.07821. lr 5.963130e-04: 100%|██████████| 34/34 [00:18<00:00,  1.84it/s]\n",
            "epoch 11 iter 33: train loss 2.03201. lr 5.955399e-04: 100%|██████████| 34/34 [00:18<00:00,  1.84it/s]\n",
            "epoch 12 iter 33: train loss 1.98853. lr 5.946939e-04: 100%|██████████| 34/34 [00:18<00:00,  1.84it/s]\n",
            "epoch 13 iter 33: train loss 1.94516. lr 5.937751e-04: 100%|██████████| 34/34 [00:18<00:00,  1.84it/s]\n",
            "epoch 14 iter 33: train loss 1.91060. lr 5.927839e-04: 100%|██████████| 34/34 [00:18<00:00,  1.84it/s]\n",
            "epoch 15 iter 33: train loss 1.86926. lr 5.917204e-04: 100%|██████████| 34/34 [00:18<00:00,  1.84it/s]\n",
            "epoch 16 iter 33: train loss 1.83663. lr 5.905849e-04: 100%|██████████| 34/34 [00:18<00:00,  1.84it/s]\n",
            "epoch 17 iter 33: train loss 1.80700. lr 5.893777e-04: 100%|██████████| 34/34 [00:18<00:00,  1.84it/s]\n",
            "epoch 18 iter 33: train loss 1.77304. lr 5.880992e-04: 100%|██████████| 34/34 [00:18<00:00,  1.83it/s]\n",
            "epoch 19 iter 33: train loss 1.75028. lr 5.867495e-04: 100%|██████████| 34/34 [00:18<00:00,  1.84it/s]\n",
            "epoch 20 iter 33: train loss 1.72336. lr 5.853291e-04: 100%|██████████| 34/34 [00:18<00:00,  1.84it/s]\n",
            "03/30/2021 18:39:41 - INFO - opt4mlPA2.mingpt.model -   number of parameters: 1.619456e+06\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "data has 1115394 characters, 65 unique.\n",
            "final epoch train loss: 1.7212978776763468\n",
            "training new model with optimizer: adamw_bias\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 1 iter 33: train loss 2.72032. lr 5.999637e-04: 100%|██████████| 34/34 [00:18<00:00,  1.84it/s]\n",
            "epoch 2 iter 33: train loss 2.52459. lr 5.998533e-04: 100%|██████████| 34/34 [00:18<00:00,  1.85it/s]\n",
            "epoch 3 iter 33: train loss 2.44581. lr 5.996690e-04: 100%|██████████| 34/34 [00:18<00:00,  1.84it/s]\n",
            "epoch 4 iter 33: train loss 2.39354. lr 5.994107e-04: 100%|██████████| 34/34 [00:18<00:00,  1.84it/s]\n",
            "epoch 5 iter 33: train loss 2.35196. lr 5.990785e-04: 100%|██████████| 34/34 [00:18<00:00,  1.84it/s]\n",
            "epoch 6 iter 33: train loss 2.29199. lr 5.986726e-04: 100%|██████████| 34/34 [00:18<00:00,  1.84it/s]\n",
            "epoch 7 iter 33: train loss 2.24424. lr 5.981929e-04: 100%|██████████| 34/34 [00:18<00:00,  1.84it/s]\n",
            "epoch 8 iter 33: train loss 2.19129. lr 5.976397e-04: 100%|██████████| 34/34 [00:18<00:00,  1.83it/s]\n",
            "epoch 9 iter 33: train loss 2.13386. lr 5.970130e-04: 100%|██████████| 34/34 [00:18<00:00,  1.84it/s]\n",
            "epoch 10 iter 33: train loss 2.08085. lr 5.963130e-04: 100%|██████████| 34/34 [00:18<00:00,  1.84it/s]\n",
            "epoch 11 iter 33: train loss 2.04818. lr 5.955399e-04: 100%|██████████| 34/34 [00:18<00:00,  1.84it/s]\n",
            "epoch 12 iter 33: train loss 1.99822. lr 5.946939e-04: 100%|██████████| 34/34 [00:18<00:00,  1.84it/s]\n",
            "epoch 13 iter 33: train loss 1.95258. lr 5.937751e-04: 100%|██████████| 34/34 [00:18<00:00,  1.85it/s]\n",
            "epoch 14 iter 33: train loss 1.91530. lr 5.927839e-04: 100%|██████████| 34/34 [00:18<00:00,  1.84it/s]\n",
            "epoch 15 iter 33: train loss 1.88442. lr 5.917204e-04: 100%|██████████| 34/34 [00:18<00:00,  1.84it/s]\n",
            "epoch 16 iter 33: train loss 1.83975. lr 5.905849e-04: 100%|██████████| 34/34 [00:18<00:00,  1.84it/s]\n",
            "epoch 17 iter 33: train loss 1.80891. lr 5.893777e-04: 100%|██████████| 34/34 [00:18<00:00,  1.84it/s]\n",
            "epoch 18 iter 33: train loss 1.78042. lr 5.880992e-04: 100%|██████████| 34/34 [00:18<00:00,  1.84it/s]\n",
            "epoch 19 iter 33: train loss 1.74491. lr 5.867495e-04: 100%|██████████| 34/34 [00:18<00:00,  1.85it/s]\n",
            "epoch 20 iter 33: train loss 1.72810. lr 5.853291e-04: 100%|██████████| 34/34 [00:18<00:00,  1.84it/s]\n",
            "03/30/2021 18:45:53 - INFO - opt4mlPA2.mingpt.model -   number of parameters: 1.619456e+06\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "data has 1115394 characters, 65 unique.\n",
            "final epoch train loss: 1.7200370781561907\n",
            "training new model with optimizer: adamw_bias_norm_scaling\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 1 iter 33: train loss 2.62770. lr 5.999637e-04: 100%|██████████| 34/34 [00:18<00:00,  1.83it/s]\n",
            "epoch 2 iter 33: train loss 2.48216. lr 5.998533e-04: 100%|██████████| 34/34 [00:18<00:00,  1.83it/s]\n",
            "epoch 3 iter 33: train loss 2.42614. lr 5.996690e-04: 100%|██████████| 34/34 [00:18<00:00,  1.83it/s]\n",
            "epoch 4 iter 33: train loss 2.38426. lr 5.994107e-04: 100%|██████████| 34/34 [00:18<00:00,  1.82it/s]\n",
            "epoch 5 iter 33: train loss 2.34164. lr 5.990785e-04: 100%|██████████| 34/34 [00:18<00:00,  1.82it/s]\n",
            "epoch 6 iter 33: train loss 2.31638. lr 5.986726e-04: 100%|██████████| 34/34 [00:18<00:00,  1.83it/s]\n",
            "epoch 7 iter 33: train loss 2.27284. lr 5.981929e-04: 100%|██████████| 34/34 [00:18<00:00,  1.83it/s]\n",
            "epoch 8 iter 33: train loss 2.20754. lr 5.976397e-04: 100%|██████████| 34/34 [00:18<00:00,  1.83it/s]\n",
            "epoch 9 iter 33: train loss 2.12293. lr 5.970130e-04: 100%|██████████| 34/34 [00:18<00:00,  1.82it/s]\n",
            "epoch 10 iter 33: train loss 2.02136. lr 5.963130e-04: 100%|██████████| 34/34 [00:18<00:00,  1.82it/s]\n",
            "epoch 11 iter 33: train loss 1.95463. lr 5.955399e-04: 100%|██████████| 34/34 [00:18<00:00,  1.82it/s]\n",
            "epoch 12 iter 33: train loss 1.86007. lr 5.946939e-04: 100%|██████████| 34/34 [00:18<00:00,  1.82it/s]\n",
            "epoch 13 iter 33: train loss 1.83119. lr 5.937751e-04: 100%|██████████| 34/34 [00:18<00:00,  1.82it/s]\n",
            "epoch 14 iter 33: train loss 1.74895. lr 5.927839e-04: 100%|██████████| 34/34 [00:18<00:00,  1.82it/s]\n",
            "epoch 15 iter 33: train loss 1.74842. lr 5.917204e-04: 100%|██████████| 34/34 [00:18<00:00,  1.83it/s]\n",
            "epoch 16 iter 33: train loss 1.68892. lr 5.905849e-04: 100%|██████████| 34/34 [00:18<00:00,  1.83it/s]\n",
            "epoch 17 iter 33: train loss 1.68376. lr 5.893777e-04: 100%|██████████| 34/34 [00:18<00:00,  1.83it/s]\n",
            "epoch 18 iter 33: train loss 1.66033. lr 5.880992e-04: 100%|██████████| 34/34 [00:18<00:00,  1.82it/s]\n",
            "epoch 19 iter 33: train loss 1.66708. lr 5.867495e-04: 100%|██████████| 34/34 [00:18<00:00,  1.83it/s]\n",
            "epoch 20 iter 33: train loss 1.63937. lr 5.853291e-04: 100%|██████████| 34/34 [00:18<00:00,  1.83it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "data has 1115394 characters, 65 unique.\n",
            "final epoch train loss: 1.6249115642379313\n",
            "best optimizer: adamw_bias_norm_scaling with loss: 1.6249115642379313\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PeDjCTOur0V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6400c39e-abeb-44c7-abbe-8bb6a9692209"
      },
      "source": [
        "# alright, let's sample some character-level shakespear\n",
        "\n",
        "context = \"O God, O God!\"\n",
        "x = torch.tensor([train_dataset.stoi[s] for s in context], dtype=torch.long)[None,...].to(trainer.device)\n",
        "y = sample(best_model, x, 2000, temperature=0.9, sample=True, top_k=5)[0]\n",
        "completion = ''.join([train_dataset.itos[int(i)] for i in y])\n",
        "print(completion)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "O God, O God! my lord, therefolf saver, think, it her, thou shall see sonds\n",
            "The have shall the sto serve where.\n",
            "\n",
            "Pray;\n",
            "And, and traith allove mercy our a parrts they hour:\n",
            "Tell see madam to hourth with some heaves being all, shere,\n",
            "The with they true heart, I he have an heaver all, as treeds what\n",
            "I that hath and he hour the with or that that head arm'd bart.\n",
            "A will he think, who shall and marrish's brother.\n",
            "\n",
            "SAMPSON:\n",
            "I have would them?\n",
            "\n",
            "CAPULINA:\n",
            "What we what she here.\n",
            "\n",
            "LADY CAPULET:\n",
            "Ah, and I am that shall thank thinks o' a that hat with the comme.\n",
            "\n",
            "PETER:\n",
            "How she sale, thou are thou words hon that that to heaver thou are them;\n",
            "When are, I have have the man of they horself the sore, and son\n",
            "Is a the son his to the days train others,\n",
            "Were and which samed the soneres them some so to all amaid our satince\n",
            "And think hand the father. He have had heave have the dare our shall will;\n",
            "And and ask horse that the somend hands in hit there.\n",
            "\n",
            "SOMERSEONT:\n",
            "I say, this he stalk, and well the deamer harged\n",
            "Inder man saidd thou with and see a presuison them;\n",
            "Is then he to so son, I will no theree?\n",
            "\n",
            "PRINCE EDWARD:\n",
            "Therefore, and therefore think of they such only,\n",
            "And which artich our hore whats, to millost with this shall being\n",
            "For a hander'd to think our will, in thinks this so the shore.\n",
            "\n",
            "LORDSER:\n",
            "I he was wear thou divill that seeeming as to to stendeer.\n",
            "\n",
            "Lord:\n",
            "Have a have the son that this, I will shall.\n",
            "\n",
            "CLARENCE:\n",
            "I was the hast arread which to stay some the stings or with\n",
            "They was a was that hou somet have sond his lie.\n",
            "\n",
            "SOMEO:\n",
            "While him I am no at worth and them; and a was a hath\n",
            "A sall her say all and mist be thou to take on the dead the with hath\n",
            "This where he seee a paster the spart ale his speak.\n",
            "Take in him then he what, this and seeps of a so contry;\n",
            "With that's he have send he thou host of thim that was heave with a\n",
            "pacrity and that all shoow spit to head was\n",
            "With at where all think to a muth same and he tath serving.\n",
            "\n",
            "PROMPEY:\n",
            "He horse stay sall a says the finds haverish'd abserove\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}